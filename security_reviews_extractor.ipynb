{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json('reviews.json')\n",
    "data[['comments']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def setup_abbr():\n",
    "    file = open(\"abbr_portuguese.txt\", encoding='utf-8')\n",
    "    abbr_dict = {}\n",
    "\n",
    "    for line in file:\n",
    "        w = line.split(\";\")\n",
    "        abbr_dict[w[0]] = w[1].replace(\"\\n\", \"\")\n",
    "    file.close()\n",
    "\n",
    "    return abbr_dict\n",
    "\n",
    "def clean(data):\n",
    "    doc = nlp(data)\n",
    "    doc_lower = doc.text.lower()\n",
    "    doc_without_emoji = emoji_pattern.sub(r'', doc_lower)\n",
    "    doc_punctuation = u\"\".join([c for c in unicodedata.normalize('NFKD', doc_without_emoji) if not unicodedata.combining(c)])\n",
    "    doc_corrected = nlp(\" \".join([abbr_dict.get(w, w) for w in doc_punctuation.split()]))\n",
    "    \n",
    "    return doc_corrected.text\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "abbr_dict = setup_abbr()\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\" # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "data['cleaned_reviews'] = data['comments'].apply(clean)\n",
    "data[['cleaned_reviews']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(text):\n",
    "    text = word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "data['tokenized_reviews'] = data['cleaned_reviews'].apply(tokenize)\n",
    "data[['tokenized_reviews']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "def stemming(text):\n",
    "    stemmer = RSLPStemmer()\n",
    "    phrase = []\n",
    "    for word in text:\n",
    "        phrase.append(stemmer.stem(word))\n",
    "    return phrase\n",
    "\n",
    "data['stem_reviews'] = data['tokenized_reviews'].apply(stemming)\n",
    "data[['stem_reviews']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def stopwordsRemove(text):\n",
    "    stop_words = stopwords.words('portuguese')\n",
    "    phrase = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            phrase.append(word)\n",
    "    return phrase\n",
    "\n",
    "data['stopwords_reviews'] = data['stem_reviews'].apply(stopwordsRemove)\n",
    "data[['stopwords_reviews']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemma = \" \"\n",
    "    for word in text:\n",
    "        lemma = wordnet_lemmatizer.lemmatize(word)\n",
    "        \n",
    "    return lemma\n",
    "\n",
    "data['lemma_reviews'] = data['stopwords_reviews'].apply(lemmatize)\n",
    "data[['lemma_reviews']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase junction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def juction(text):\n",
    "    phrase = []\n",
    "    for word in text:\n",
    "        phrase.append(word)\n",
    "    \n",
    "    phraseStr = ' '.join(phrase)\n",
    "    return phraseStr\n",
    "\n",
    "data['junction'] = data['stopwords_reviews'].apply(juction)\n",
    "data[['junction']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "def informationExtraction(text):\n",
    "    evaluations = []\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    securityTerms = ['segur', 'roub', 'clon']\n",
    "    patterns = [nlp(term) for term in securityTerms]\n",
    "    \n",
    "    #Ver utilização do add_patterns juntamente com o entity_ruler (otimização)\n",
    "    matcher = PhraseMatcher(nlp.vocab) \n",
    "    matcher.add(\"SECURITY_PATTERN\", patterns)\n",
    "    \n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    for i in range(0,len(matches)):\n",
    "        token = doc[matches[i][1]:matches[i][2]]\n",
    "        evaluations.append(str(token))\n",
    "            \n",
    "    return evaluations\n",
    "\n",
    "data['extracted_reviews'] = data['junction'].apply(informationExtraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    if len(data.loc[i,'extracted_reviews'])!=0:\n",
    "        print(data.loc[i,'cleaned_reviews'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
