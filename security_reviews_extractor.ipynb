{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "data = pd.read_json('allreviews.json')\n",
    "data.describe()\n",
    "#data.loc[:,[\"text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "def setup_abbr():\n",
    "    file = open(\"abbr_portuguese.txt\", encoding='utf-8')\n",
    "    abbr_dict = {}\n",
    "\n",
    "    for line in file:\n",
    "        w = line.split(\";\")\n",
    "        abbr_dict[w[0]] = w[1].replace(\"\\n\", \"\")\n",
    "    file.close()\n",
    "\n",
    "    return abbr_dict\n",
    "\n",
    "def clean(data):\n",
    "    doc = nlp(data)\n",
    "    doc_lower = doc.text.lower()\n",
    "    punct = string.punctuation\n",
    "    for c in punct:\n",
    "        doc_lower = doc_lower.replace(c, \"\")\n",
    "    doc_without_emoji = emoji_pattern.sub(r'', doc_lower)\n",
    "    doc_punctuation = u\"\".join([c for c in unicodedata.normalize('NFKD', doc_without_emoji) if not unicodedata.combining(c)])\n",
    "    doc_corrected = nlp(\" \".join([abbr_dict.get(w, w) for w in doc_punctuation.split()]))\n",
    "    \n",
    "    return doc_corrected.text\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "abbr_dict = setup_abbr()\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\" # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "data['cleaned_reviews'] = data['text'].apply(clean)\n",
    "#data.loc[:,[\"cleaned_reviews\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def wordTag(text):\n",
    "    tagger = joblib.load('POS_tagger_brill.pkl')\n",
    "    text = tagger.tag(word_tokenize(text))\n",
    "    return text\n",
    " \n",
    "data['tag_reviews'] = data['cleaned_reviews'].apply(wordTag)\n",
    "data.loc[:,[\"cleaned_reviews\", \"tag_reviews\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(text):\n",
    "    text = word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "data['tokenized_reviews'] = data['cleaned_reviews'].apply(tokenize)\n",
    "#data.loc[:,[\"tokenized_reviews\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "\n",
    "def stopwordsRemove(text):\n",
    "    STOP_WORDS.update(['nao', 'sim', 'caixa', 'nubank', 'aplicativo', 'dinheiro', 'acessar', 'consigo', 'banco', 'email', 'pra', 'pro', 'ta', 'ja', 'so', 'fica'])\n",
    "    stop_words = list(set(stopwords.words('portuguese') + list(STOP_WORDS)))\n",
    "    phrase = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            phrase.append(word)\n",
    "    return phrase\n",
    "\n",
    "data['stopwords_reviews'] = data['tokenized_reviews'].apply(stopwordsRemove)\n",
    "#data.loc[:,[\"stopwords_reviews\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "dataList = data['stopwords_reviews'].to_list()\n",
    "\n",
    "words = []\n",
    "\n",
    "for i in range(len(dataList)):\n",
    "    for j in range(len(dataList[i])):\n",
    "        words.append(dataList[i][j])\n",
    "\n",
    "fdist = nltk.FreqDist(words).most_common(10)\n",
    "\n",
    "mostCommon = []\n",
    "\n",
    "for word, frequency in fdist:\n",
    "    mostCommon.append(word)\n",
    "\n",
    "def stemming(text):\n",
    "    stemmer = RSLPStemmer()\n",
    "    phrase = []\n",
    "    \n",
    "    for word in text:\n",
    "        phrase.append(stemmer.stem(word))\n",
    "        \n",
    "    return phrase    \n",
    "\n",
    "print(mostCommon)\n",
    "\n",
    "mostCommonStem = stemming(mostCommon)\n",
    "\n",
    "print(mostCommonStem)\n",
    "\n",
    "fdist = pd.Series(dict(fdist))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "barplot = sns.barplot(x=fdist.index, y=fdist.values, ax=ax)\n",
    "plt.title('Frequency Word')\n",
    "plt.xticks(rotation=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "def stemming(text):\n",
    "    stemmer = RSLPStemmer()\n",
    "    phrase = []\n",
    "    for word in text:\n",
    "        phrase.append(stemmer.stem(word))\n",
    "    return phrase\n",
    "\n",
    "data['stem_reviews'] = data['tokenized_reviews'].apply(stemming)\n",
    "#data.loc[:,[\"stem_reviews\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemma = \" \"\n",
    "    for word in text:\n",
    "        lemma = wordnet_lemmatizer.lemmatize(word)\n",
    "        \n",
    "    return lemma\n",
    "\n",
    "data['lemma_reviews'] = data['stopwords_reviews'].apply(lemmatize)\n",
    "data.loc[:,[\"lemma_reviews\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase junction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def juction(text):\n",
    "    phrase = []\n",
    "    for word in text:\n",
    "        phrase.append(word)\n",
    "    \n",
    "    phraseStr = ' '.join(phrase)\n",
    "    return phraseStr\n",
    "\n",
    "data['junction'] = data['stem_reviews'].apply(juction)\n",
    "data.loc[:,[\"junction\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Security posts extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "def securityReviewsClassifier(text):\n",
    "    securityReviews = []\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    securityTerms = mostCommonStem\n",
    "    patterns = [nlp(term) for term in securityTerms]\n",
    "    \n",
    "    matcher = PhraseMatcher(nlp.vocab) \n",
    "    matcher.add(\"SECURITY_PATTERN\", patterns)\n",
    "    \n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    for i in range(0,len(matches)):\n",
    "        token = doc[matches[i][1]:matches[i][2]]\n",
    "        securityReviews.append(str(token))\n",
    "            \n",
    "    return securityReviews\n",
    "\n",
    "def class1(text):\n",
    "    class1Reviews = []\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    class1Terms = ['senh', 'acess']\n",
    "    patterns = [nlp(term) for term in class1Terms]\n",
    "    \n",
    "    matcher = PhraseMatcher(nlp.vocab) \n",
    "    matcher.add(\"SECURITY_PATTERN\", patterns)\n",
    "    \n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    for i in range(0,len(matches)):\n",
    "        token = doc[matches[i][1]:matches[i][2]]\n",
    "        class1Reviews.append(str(token))\n",
    "            \n",
    "    return class1Reviews\n",
    "\n",
    "def class2(text):\n",
    "    class2Reviews = []\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    class2Terms = ['assinat', 'eletron', 'biometr', 'reconhec', 'fac']\n",
    "    patterns = [nlp(term) for term in class2Terms]\n",
    "    \n",
    "    matcher = PhraseMatcher(nlp.vocab) \n",
    "    matcher.add(\"SECURITY_PATTERN\", patterns)\n",
    "    \n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    for i in range(0,len(matches)):\n",
    "        token = doc[matches[i][1]:matches[i][2]]\n",
    "        class2Terms.append(str(token))\n",
    "            \n",
    "    return class2Reviews\n",
    "\n",
    "def class3(text):\n",
    "    class3Reviews = []\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    class3Terms = ['bloquei', 'chav']\n",
    "    patterns = [nlp(term) for term in class3Terms]\n",
    "    \n",
    "    matcher = PhraseMatcher(nlp.vocab) \n",
    "    matcher.add(\"SECURITY_PATTERN\", patterns)\n",
    "    \n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    for i in range(0,len(matches)):\n",
    "        token = doc[matches[i][1]:matches[i][2]]\n",
    "        class3Reviews.append(str(token))\n",
    "            \n",
    "    return class3Reviews\n",
    "\n",
    "def class4(text):\n",
    "    class4Reviews = []\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    class4Terms = ['fraud', 'golp', 'clon', 'roub']\n",
    "    patterns = [nlp(term) for term in class4Terms]\n",
    "    \n",
    "    matcher = PhraseMatcher(nlp.vocab) \n",
    "    matcher.add(\"SECURITY_PATTERN\", patterns)\n",
    "    \n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    for i in range(0,len(matches)):\n",
    "        token = doc[matches[i][1]:matches[i][2]]\n",
    "        class4Reviews.append(str(token))\n",
    "            \n",
    "    return class4Reviews\n",
    "\n",
    "data['security_reviews'] = data['junction'].apply(securityReviewsClassifier)\n",
    "# data['class1'] = data['junction'].apply(class1)\n",
    "# data['class2'] = data['junction'].apply(class2)\n",
    "# data['class3'] = data['junction'].apply(class3)\n",
    "# data['class4'] = data['junction'].apply(class4)\n",
    "# data.loc[:, 'class1']\n",
    "# data.loc[:, 'class2']\n",
    "# data.loc[:, 'class3']\n",
    "# data.loc[:, 'class4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "extracted = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if len(data.loc[i,'security_reviews'])!=0:\n",
    "        extracted.append(data.loc[i,'cleaned_reviews'])\n",
    "        \n",
    "dfExtracted = pd.DataFrame(extracted, columns=[\"reviews_classified_all\"])\n",
    "dfExtracted.to_csv(\"csvresult_all.csv\", columns = [\"reviews_classified_all\"])\n",
    "\n",
    "#data.to_csv(\"csvresult_all.csv\", columns = [\"security_reviews\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "def stemming(text):\n",
    "    stemmer = RSLPStemmer()\n",
    "    phrase = []\n",
    "    \n",
    "    for word in text:\n",
    "        phrase.append(stemmer.stem(word))\n",
    "        \n",
    "    return phrase \n",
    "\n",
    "words = ['acesso', 'senha', 'assinatura', 'eletronica', 'biometria', 'reconhecimento', 'facial', 'bloqueio', 'chave', 'fraude', 'golpe', 'clonagem', 'roubo']\n",
    "\n",
    "stem = stemming(words)\n",
    "\n",
    "print(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
