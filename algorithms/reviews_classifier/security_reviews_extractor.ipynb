{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>por algum motivo o aplicativo nao esta querend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text\n",
       "count                                                 240\n",
       "unique                                                240\n",
       "top     por algum motivo o aplicativo nao esta querend...\n",
       "freq                                                    1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "data = pd.read_json('jsons/reviews_class_nubank.json')\n",
    "data.describe()\n",
    "#data.loc[:,[\"text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "def setup_abbr():\n",
    "    file = open(\"abbr_portuguese.txt\", encoding='utf-8')\n",
    "    abbr_dict = {}\n",
    "\n",
    "    for line in file:\n",
    "        w = line.split(\";\")\n",
    "        abbr_dict[w[0]] = w[1].replace(\"\\n\", \"\")\n",
    "    file.close()\n",
    "\n",
    "    return abbr_dict\n",
    "\n",
    "def clean(data):\n",
    "    doc = nlp(data)\n",
    "    doc_lower = doc.text.lower()\n",
    "    punct = string.punctuation\n",
    "    for c in punct:\n",
    "        doc_lower = doc_lower.replace(c, \"\")\n",
    "    doc_without_emoji = emoji_pattern.sub(r'', doc_lower)\n",
    "    doc_punctuation = u\"\".join([c for c in unicodedata.normalize('NFKD', doc_without_emoji) if not unicodedata.combining(c)])\n",
    "    doc_corrected = nlp(\" \".join([abbr_dict.get(w, w) for w in doc_punctuation.split()]))\n",
    "    \n",
    "    return doc_corrected.text\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "abbr_dict = setup_abbr()\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\" # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "data['cleaned_reviews'] = data['text'].apply(clean)\n",
    "#data.loc[:,[\"cleaned_reviews\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_reviews</th>\n",
       "      <th>tag_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>por algum motivo o aplicativo nao esta querend...</td>\n",
       "      <td>[(por, PREP), (algum, PROADJ), (motivo, N), (o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>o aplicativo tem muito problema e bug faz um m...</td>\n",
       "      <td>[(o, ART), (aplicativo, N), (tem, V), (muito, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a nubank veio piorando muito esses dias agora ...</td>\n",
       "      <td>[(a, ART), (nubank, NPROP), (veio, V), (pioran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amo usar o nubank e pratico de facil acesso in...</td>\n",
       "      <td>[(amo, N), (usar, V), (o, ART), (nubank, NPROP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pessimo banco recebi um bloqueio do nada por s...</td>\n",
       "      <td>[(pessimo, ADJ), (banco, N), (recebi, V), (um,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>pior analise de credito que ha no mercado nao ...</td>\n",
       "      <td>[(pior, ADJ), (analise, V), (de, PREP), (credi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>ao pressionar para colar a chave do pix o apli...</td>\n",
       "      <td>[(ao, PREP), (pressionar, V), (para, PREP), (c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>a entrada no aplicativo digitando o cpf e muit...</td>\n",
       "      <td>[(a, ART), (entrada, N), (no, PREP), (aplicati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>ja tem um tempo tinha esquecido porque da outr...</td>\n",
       "      <td>[(ja, N), (tem, V), (um, ART), (tempo, N), (ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>facilidade no acesso facilidade em pagamentos ...</td>\n",
       "      <td>[(facilidade, N), (no, PREP), (acesso, N), (fa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       cleaned_reviews  \\\n",
       "0    por algum motivo o aplicativo nao esta querend...   \n",
       "1    o aplicativo tem muito problema e bug faz um m...   \n",
       "2    a nubank veio piorando muito esses dias agora ...   \n",
       "3    amo usar o nubank e pratico de facil acesso in...   \n",
       "4    pessimo banco recebi um bloqueio do nada por s...   \n",
       "..                                                 ...   \n",
       "235  pior analise de credito que ha no mercado nao ...   \n",
       "236  ao pressionar para colar a chave do pix o apli...   \n",
       "237  a entrada no aplicativo digitando o cpf e muit...   \n",
       "238  ja tem um tempo tinha esquecido porque da outr...   \n",
       "239  facilidade no acesso facilidade em pagamentos ...   \n",
       "\n",
       "                                           tag_reviews  \n",
       "0    [(por, PREP), (algum, PROADJ), (motivo, N), (o...  \n",
       "1    [(o, ART), (aplicativo, N), (tem, V), (muito, ...  \n",
       "2    [(a, ART), (nubank, NPROP), (veio, V), (pioran...  \n",
       "3    [(amo, N), (usar, V), (o, ART), (nubank, NPROP...  \n",
       "4    [(pessimo, ADJ), (banco, N), (recebi, V), (um,...  \n",
       "..                                                 ...  \n",
       "235  [(pior, ADJ), (analise, V), (de, PREP), (credi...  \n",
       "236  [(ao, PREP), (pressionar, V), (para, PREP), (c...  \n",
       "237  [(a, ART), (entrada, N), (no, PREP), (aplicati...  \n",
       "238  [(ja, N), (tem, V), (um, ART), (tempo, N), (ti...  \n",
       "239  [(facilidade, N), (no, PREP), (acesso, N), (fa...  \n",
       "\n",
       "[240 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def wordTag(text):\n",
    "    tagger = joblib.load('POS_tagger_brill.pkl')\n",
    "    text = tagger.tag(word_tokenize(text))\n",
    "    return text\n",
    " \n",
    "data['tag_reviews'] = data['cleaned_reviews'].apply(wordTag)\n",
    "data.loc[:,[\"cleaned_reviews\", \"tag_reviews\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[por, algum, motivo, o, aplicativo, nao, esta,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[o, aplicativo, tem, muito, problema, e, bug, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[a, nubank, veio, piorando, muito, esses, dias...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[amo, usar, o, nubank, e, pratico, de, facil, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[pessimo, banco, recebi, um, bloqueio, do, nad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>[pior, analise, de, credito, que, ha, no, merc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>[ao, pressionar, para, colar, a, chave, do, pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>[a, entrada, no, aplicativo, digitando, o, cpf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>[ja, tem, um, tempo, tinha, esquecido, porque,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>[facilidade, no, acesso, facilidade, em, pagam...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     tokenized_reviews\n",
       "0    [por, algum, motivo, o, aplicativo, nao, esta,...\n",
       "1    [o, aplicativo, tem, muito, problema, e, bug, ...\n",
       "2    [a, nubank, veio, piorando, muito, esses, dias...\n",
       "3    [amo, usar, o, nubank, e, pratico, de, facil, ...\n",
       "4    [pessimo, banco, recebi, um, bloqueio, do, nad...\n",
       "..                                                 ...\n",
       "235  [pior, analise, de, credito, que, ha, no, merc...\n",
       "236  [ao, pressionar, para, colar, a, chave, do, pi...\n",
       "237  [a, entrada, no, aplicativo, digitando, o, cpf...\n",
       "238  [ja, tem, um, tempo, tinha, esquecido, porque,...\n",
       "239  [facilidade, no, acesso, facilidade, em, pagam...\n",
       "\n",
       "[240 rows x 1 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(text):\n",
    "    text = word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "data['tokenized_reviews'] = data['cleaned_reviews'].apply(tokenize)\n",
    "data.loc[:,[\"tokenized_reviews\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stopwords_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[algum, motivo, querendo, abrir, celular, mand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[problema, bug, mes, tento, mudaram, entrar, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[veio, piorando, dias, voce, colocar, cartao, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[amo, pratico, facil, acesso, interface, linda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[pessimo, recebi, bloqueio, simplesmente, cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>[pior, analise, credito, ha, mercado, levam, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>[pressionar, colar, chave, pix, demora, aciona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>[entrada, digitando, cpf, ruim, deveria, atrav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>[esquecido, ate, desisti, tento, conta, pj, mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>[facilidade, acesso, facilidade, pagamentos, d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     stopwords_reviews\n",
       "0    [algum, motivo, querendo, abrir, celular, mand...\n",
       "1    [problema, bug, mes, tento, mudaram, entrar, d...\n",
       "2    [veio, piorando, dias, voce, colocar, cartao, ...\n",
       "3    [amo, pratico, facil, acesso, interface, linda...\n",
       "4    [pessimo, recebi, bloqueio, simplesmente, cont...\n",
       "..                                                 ...\n",
       "235  [pior, analise, credito, ha, mercado, levam, c...\n",
       "236  [pressionar, colar, chave, pix, demora, aciona...\n",
       "237  [entrada, digitando, cpf, ruim, deveria, atrav...\n",
       "238  [esquecido, ate, desisti, tento, conta, pj, mo...\n",
       "239  [facilidade, acesso, facilidade, pagamentos, d...\n",
       "\n",
       "[240 rows x 1 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "\n",
    "def stopwordsRemove(text):\n",
    "    STOP_WORDS.update(['nao', 'sim', 'caixa', 'nubank', 'aplicativo', 'dinheiro', 'acessar', 'consigo', 'banco', 'email', 'pra', 'pro', 'ta', 'ja', 'so', 'fica'])\n",
    "    stop_words = list(set(stopwords.words('portuguese') + list(STOP_WORDS)))\n",
    "    phrase = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            phrase.append(word)\n",
    "    return phrase\n",
    "\n",
    "data['stopwords_reviews'] = data['tokenized_reviews'].apply(stopwordsRemove)\n",
    "data.loc[:,[\"stopwords_reviews\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to /Users/moabsouza/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('rslp')\n",
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "def stemming(text):\n",
    "    stemmer = RSLPStemmer()\n",
    "    phrase = []\n",
    "    for word in text:\n",
    "        phrase.append(stemmer.stem(word))\n",
    "    return phrase\n",
    "\n",
    "data['stem_reviews'] = data['tokenized_reviews'].apply(stemming)\n",
    "#data.loc[:,[\"stem_reviews\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/moabsouza/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>conta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>piorar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acesso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>duvidas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>prendem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>excelencia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>comando</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>pronto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>lixo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>adoro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    lemma_reviews\n",
       "0           conta\n",
       "1          piorar\n",
       "2          acesso\n",
       "3         duvidas\n",
       "4         prendem\n",
       "..            ...\n",
       "235    excelencia\n",
       "236       comando\n",
       "237        pronto\n",
       "238          lixo\n",
       "239         adoro\n",
       "\n",
       "[240 rows x 1 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemma = \" \"\n",
    "    for word in text:\n",
    "        lemma = wordnet_lemmatizer.lemmatize(word)\n",
    "        \n",
    "    return lemma\n",
    "\n",
    "data['lemma_reviews'] = data['stopwords_reviews'].apply(lemmatize)\n",
    "data.loc[:,[\"lemma_reviews\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase junction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>junction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>por algum motiv o aplic nao est quer abr no me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>o aplic tem muit problem e bug faz um me ja qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a nubank vei pior muit ess dia agor voc tem qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amo us o nubank e pra de facil acess interfac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pess banc receb um bloquei do nad por simples ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>pi analis de credit que ha no merc nao lev em ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>ao press par col a chav do pix o aplic dem par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>a entr no aplic digit o cpf e muit ruim dev se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>ja tem um temp tinh esquec porqu da outr vez e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>facil no acess facil em pag e na descrica de t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              junction\n",
       "0    por algum motiv o aplic nao est quer abr no me...\n",
       "1    o aplic tem muit problem e bug faz um me ja qu...\n",
       "2    a nubank vei pior muit ess dia agor voc tem qu...\n",
       "3    amo us o nubank e pra de facil acess interfac ...\n",
       "4    pess banc receb um bloquei do nad por simples ...\n",
       "..                                                 ...\n",
       "235  pi analis de credit que ha no merc nao lev em ...\n",
       "236  ao press par col a chav do pix o aplic dem par...\n",
       "237  a entr no aplic digit o cpf e muit ruim dev se...\n",
       "238  ja tem um temp tinh esquec porqu da outr vez e...\n",
       "239  facil no acess facil em pag e na descrica de t...\n",
       "\n",
       "[240 rows x 1 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def juction(text):\n",
    "    phrase = []\n",
    "    for word in text:\n",
    "        phrase.append(word)\n",
    "    \n",
    "    phraseStr = ' '.join(phrase)\n",
    "    return phraseStr\n",
    "\n",
    "data['junction'] = data['stem_reviews'].apply(juction)\n",
    "data.loc[:,[\"junction\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Security posts extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "def securityReviewsClassifier(text):\n",
    "    securityReviews = []\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    securityTerms = mostCommonStem\n",
    "    patterns = [nlp(term) for term in securityTerms]\n",
    "    \n",
    "    matcher = PhraseMatcher(nlp.vocab) \n",
    "    matcher.add(\"SECURITY_PATTERN\", patterns)\n",
    "    \n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    for i in range(0,len(matches)):\n",
    "        token = doc[matches[i][1]:matches[i][2]]\n",
    "        securityReviews.append(str(token))\n",
    "            \n",
    "    return securityReviews\n",
    "\n",
    "def class1(text):\n",
    "    class1Reviews = []\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    class1Terms = ['senh', 'acess']\n",
    "    patterns = [nlp(term) for term in class1Terms]\n",
    "    \n",
    "    matcher = PhraseMatcher(nlp.vocab) \n",
    "    matcher.add(\"SECURITY_PATTERN\", patterns)\n",
    "    \n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    for i in range(0,len(matches)):\n",
    "        token = doc[matches[i][1]:matches[i][2]]\n",
    "        class1Reviews.append(str(token))\n",
    "            \n",
    "    return class1Reviews\n",
    "\n",
    "def class2(text):\n",
    "    class2Reviews = []\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    class2Terms = ['assinat', 'eletron', 'biometr', 'reconhec', 'fac']\n",
    "    patterns = [nlp(term) for term in class2Terms]\n",
    "    \n",
    "    matcher = PhraseMatcher(nlp.vocab) \n",
    "    matcher.add(\"SECURITY_PATTERN\", patterns)\n",
    "    \n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    for i in range(0,len(matches)):\n",
    "        token = doc[matches[i][1]:matches[i][2]]\n",
    "        class2Terms.append(str(token))\n",
    "            \n",
    "    return class2Reviews\n",
    "\n",
    "def class3(text):\n",
    "    class3Reviews = []\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    class3Terms = ['bloquei', 'chav']\n",
    "    patterns = [nlp(term) for term in class3Terms]\n",
    "    \n",
    "    matcher = PhraseMatcher(nlp.vocab) \n",
    "    matcher.add(\"SECURITY_PATTERN\", patterns)\n",
    "    \n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    for i in range(0,len(matches)):\n",
    "        token = doc[matches[i][1]:matches[i][2]]\n",
    "        class3Reviews.append(str(token))\n",
    "            \n",
    "    return class3Reviews\n",
    "\n",
    "def class4(text):\n",
    "    class4Reviews = []\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    class4Terms = ['fraud', 'golp', 'clon', 'roub']\n",
    "    patterns = [nlp(term) for term in class4Terms]\n",
    "    \n",
    "    matcher = PhraseMatcher(nlp.vocab) \n",
    "    matcher.add(\"SECURITY_PATTERN\", patterns)\n",
    "    \n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    for i in range(0,len(matches)):\n",
    "        token = doc[matches[i][1]:matches[i][2]]\n",
    "        class4Reviews.append(str(token))\n",
    "            \n",
    "    return class4Reviews\n",
    "\n",
    "# data['security_reviews'] = data['junction'].apply(securityReviewsClassifier)\n",
    "# data['class1'] = data['junction'].apply(class1)\n",
    "# data['class2'] = data['junction'].apply(class2)\n",
    "# data['class3'] = data['junction'].apply(class3)\n",
    "data['class4'] = data['junction'].apply(class4)\n",
    "# data.loc[:, 'class1']\n",
    "# data.loc[:, 'class2']\n",
    "# data.loc[:, 'class3']\n",
    "# data.loc[:, 'class4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "extracted = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if len(data.loc[i,'class4'])!=0:\n",
    "        extracted.append(data.loc[i,'cleaned_reviews'])\n",
    "        \n",
    "dfExtracted = pd.DataFrame(extracted, columns=[\"reviews_classified_all\"])\n",
    "dfExtracted.to_csv(\"csvresult_class4_caixa.csv\", columns = [\"reviews_classified_all\"])\n",
    "\n",
    "#data.to_csv(\"csvresult_all.csv\", columns = [\"security_reviews\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mrslp\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('rslp')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mstemmers/rslp/step0.pt\u001b[0m\n\n  Searched in:\n    - '/Users/moabsouza/nltk_data'\n    - '/opt/homebrew/opt/python@3.11/Frameworks/Python.framework/Versions/3.11/nltk_data'\n    - '/opt/homebrew/opt/python@3.11/Frameworks/Python.framework/Versions/3.11/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.11/Frameworks/Python.framework/Versions/3.11/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/moabsouza/Development/TCC/reviews-classifier-algorithm/algorithms/reviews_classifier/security_reviews_extractor.ipynb Célula 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/moabsouza/Development/TCC/reviews-classifier-algorithm/algorithms/reviews_classifier/security_reviews_extractor.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m phrase \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/moabsouza/Development/TCC/reviews-classifier-algorithm/algorithms/reviews_classifier/security_reviews_extractor.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m words \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39macesso\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msenha\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39massinatura\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39meletronica\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbiometria\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mreconhecimento\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfacial\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbloqueio\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mchave\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfraude\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgolpe\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mclonagem\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mroubo\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/moabsouza/Development/TCC/reviews-classifier-algorithm/algorithms/reviews_classifier/security_reviews_extractor.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m stem \u001b[39m=\u001b[39m stemming(words)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/moabsouza/Development/TCC/reviews-classifier-algorithm/algorithms/reviews_classifier/security_reviews_extractor.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(stem)\n",
      "\u001b[1;32m/Users/moabsouza/Development/TCC/reviews-classifier-algorithm/algorithms/reviews_classifier/security_reviews_extractor.ipynb Célula 20\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/moabsouza/Development/TCC/reviews-classifier-algorithm/algorithms/reviews_classifier/security_reviews_extractor.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstemming\u001b[39m(text):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/moabsouza/Development/TCC/reviews-classifier-algorithm/algorithms/reviews_classifier/security_reviews_extractor.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     stemmer \u001b[39m=\u001b[39m RSLPStemmer()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/moabsouza/Development/TCC/reviews-classifier-algorithm/algorithms/reviews_classifier/security_reviews_extractor.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     phrase \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/moabsouza/Development/TCC/reviews-classifier-algorithm/algorithms/reviews_classifier/security_reviews_extractor.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m text:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/nltk/stem/rslp.py:56\u001b[0m, in \u001b[0;36mRSLPStemmer.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     54\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_rule(\u001b[39m\"\u001b[39;49m\u001b[39mstep0.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m     57\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_rule(\u001b[39m\"\u001b[39m\u001b[39mstep1.pt\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_rule(\u001b[39m\"\u001b[39m\u001b[39mstep2.pt\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/nltk/stem/rslp.py:65\u001b[0m, in \u001b[0;36mRSLPStemmer.read_rule\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_rule\u001b[39m(\u001b[39mself\u001b[39m, filename):\n\u001b[0;32m---> 65\u001b[0m     rules \u001b[39m=\u001b[39m load(\u001b[39m\"\u001b[39;49m\u001b[39mnltk:stemmers/rslp/\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m filename, \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mraw\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m     lines \u001b[39m=\u001b[39m rules\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m     lines \u001b[39m=\u001b[39m [line \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines \u001b[39mif\u001b[39;00m line \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m]  \u001b[39m# remove blank lines\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[1;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mrslp\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('rslp')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mstemmers/rslp/step0.pt\u001b[0m\n\n  Searched in:\n    - '/Users/moabsouza/nltk_data'\n    - '/opt/homebrew/opt/python@3.11/Frameworks/Python.framework/Versions/3.11/nltk_data'\n    - '/opt/homebrew/opt/python@3.11/Frameworks/Python.framework/Versions/3.11/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.11/Frameworks/Python.framework/Versions/3.11/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "def stemming(text):\n",
    "    stemmer = RSLPStemmer()\n",
    "    phrase = []\n",
    "    \n",
    "    for word in text:\n",
    "        phrase.append(stemmer.stem(word))\n",
    "        \n",
    "    return phrase \n",
    "\n",
    "words = ['acesso', 'senha', 'assinatura', 'eletronica', 'biometria', 'reconhecimento', 'facial', 'bloqueio', 'chave', 'fraude', 'golpe', 'clonagem', 'roubo']\n",
    "\n",
    "stem = stemming(words)\n",
    "\n",
    "print(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
